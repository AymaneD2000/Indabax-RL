#!/usr/bin/env python3
"""
SOLUTIONS COMPL√àTES - Agent CNN pour Deep Q-Learning bas√© sur les Images
"""

import os
import sys
import numpy as np
import tensorflow as tf
from collections import deque
import random
from game import SnakeGameAI2
import matplotlib.pyplot as plt
import cv2

class SimpleDQNAgent:
    """Agent DQN avec CNN pour l'apprentissage par renforcement bas√© sur les images"""
    
    def __init__(self, 
                 state_shape=(84, 84, 4),
                 action_size=3,
                 learning_rate=0.0005,
                 gamma=0.99,
                 epsilon_start=1.0,
                 epsilon_end=0.01,
                 epsilon_decay_episodes=600,
                 memory_size=1000,
                 batch_size=32,
                 target_update_freq=100,
                 min_replay_size=300):
        
        self.state_shape = state_shape
        self.action_size = action_size
        self.learning_rate = learning_rate
        self.gamma = gamma
        self.epsilon_start = epsilon_start
        self.epsilon_end = epsilon_end
        self.epsilon_decay_episodes = epsilon_decay_episodes
        self.batch_size = batch_size
        self.target_update_freq = target_update_freq
        self.min_replay_size = min_replay_size
        
        # Construction des r√©seaux
        self.main_network = self._build_network()
        self.target_network = self._build_network()
        self.update_target_network()
        
        # Optimiseur
        self.optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)
        
        # M√©moire pour l'experience replay
        self.memory = deque(maxlen=memory_size)
        
        # Variables d'entra√Ænement
        self.episode_count = 0
        self.step_count = 0
        self.epsilon = epsilon_start
        self.losses = []
        self.scores = []
        self.avg_scores = []
        
    def _build_network(self):
        """
        SOLUTION EXERCICE 1 - ARCHITECTURE CNN COMPL√àTE
        """
        model = tf.keras.Sequential([
            tf.keras.layers.Input(shape=self.state_shape),
            
            # Normalisation des pixels (0-255 ‚Üí 0-1)
            tf.keras.layers.Lambda(lambda x: tf.cast(x, tf.float32) / 255.0),
            
            # Couches convolutionnelles pour extraction de features
            tf.keras.layers.Conv2D(16, 8, strides=4, activation='relu'),
            tf.keras.layers.Conv2D(32, 4, strides=2, activation='relu'),
            tf.keras.layers.Conv2D(32, 3, strides=1, activation='relu'),
            
            # Aplatir pour les couches denses
            tf.keras.layers.Flatten(),
            
            # Couches denses pour la d√©cision
            tf.keras.layers.Dense(256, activation='relu'),
            tf.keras.layers.Dense(self.action_size, activation='linear')
        ])
        
        return model
    
    def update_target_network(self):
        """Copie les poids du r√©seau principal vers le r√©seau cible"""
        self.target_network.set_weights(self.main_network.get_weights())
    
    def get_epsilon(self):
        """Calcule la valeur actuelle d'epsilon pour l'exploration"""
        if self.episode_count < self.epsilon_decay_episodes:
            epsilon = self.epsilon_start - (self.epsilon_start - self.epsilon_end) * \
                     (self.episode_count / self.epsilon_decay_episodes)
        else:
            epsilon = self.epsilon_end
        return epsilon
    
    def preprocess_state(self, state):
        """
        SOLUTION EXERCICE 2 - PR√âPROCESSING DES IMAGES
        """
        # V√©rifications de base
        if state is None:
            raise ValueError("√âtat d'entr√©e ne peut pas √™tre None")
        
        # V√©rifier la forme
        if state.shape != self.state_shape:
            raise ValueError(f"Forme attendue {self.state_shape}, re√ßue {state.shape}")
        
        # Convertir en float32 et normaliser si n√©cessaire
        state = state.astype(np.float32)
        
        # Si les valeurs sont entre 0-255, les normaliser
        if np.max(state) > 1.0:
            state = state / 255.0
        
        return state
    
    def act(self, state):
        """
        SOLUTION EXERCICE 3 - STRAT√âGIE EPSILON-GREEDY POUR IMAGES
        """
        # Calculer epsilon
        self.epsilon = self.get_epsilon()
        
        if np.random.random() < self.epsilon:
            # Exploration - action al√©atoire
            return np.random.randint(0, self.action_size)
        else:
            # Exploitation - utiliser le r√©seau pour pr√©dire
            
            # Pr√©processer l'√©tat
            processed_state = self.preprocess_state(state)
            
            # Ajouter dimension batch
            state_tensor = tf.constant(np.expand_dims(processed_state, axis=0))
            
            # Pr√©dire les Q-values
            q_values = self.main_network(state_tensor, training=False)
            
            # Choisir l'action avec la plus haute Q-value
            return int(tf.argmax(q_values[0]).numpy())
    
    def remember(self, state, action, reward, next_state, done):
        """Stocker l'exp√©rience dans la m√©moire"""
        self.memory.append((state, action, reward, next_state, done))
    
    def train(self):
        """
        SOLUTION EXERCICE 4 - ENTRA√éNEMENT CNN AVEC EXPERIENCE REPLAY
        """
        if len(self.memory) < self.min_replay_size:
            return None
        
        # √âchantillonnage du batch
        batch = random.sample(self.memory, min(len(self.memory), self.batch_size))
        
        # Pr√©parer les donn√©es du batch
        states = np.array([e[0] for e in batch], dtype=np.float32)
        actions = np.array([e[1] for e in batch], dtype=np.int32)
        rewards = np.array([e[2] for e in batch], dtype=np.float32)
        next_states = np.array([e[3] for e in batch], dtype=np.float32)
        dones = np.array([e[4] for e in batch], dtype=np.bool_)
        
        # Convertir en tenseurs TensorFlow
        states_tensor = tf.constant(states)
        next_states_tensor = tf.constant(next_states)
        
        # Entra√Ænement avec GradientTape
        with tf.GradientTape() as tape:
            # Q-values actuelles du r√©seau principal
            current_q_values = self.main_network(states_tensor, training=True)
            
            # S√©lectionner les Q-values pour les actions prises
            batch_indices = tf.range(tf.shape(actions)[0], dtype=tf.int32)
            action_indices = tf.stack([batch_indices, actions], axis=1)
            current_q_values = tf.gather_nd(current_q_values, action_indices)
            
            # Q-values suivantes du r√©seau cible
            next_q_values = self.target_network(next_states_tensor, training=False)
            max_next_q_values = tf.reduce_max(next_q_values, axis=1)
            
            # Q-values cibles (√©quation de Bellman)
            target_q_values = rewards + (self.gamma * max_next_q_values * (1.0 - tf.cast(dones, tf.float32)))
            
            # Calculer la perte
            loss = tf.keras.losses.MeanSquaredError()(target_q_values, current_q_values)
        
        # Appliquer les gradients
        gradients = tape.gradient(loss, self.main_network.trainable_variables)
        self.optimizer.apply_gradients(zip(gradients, self.main_network.trainable_variables))
        
        # Mise √† jour du r√©seau cible
        if self.step_count % self.target_update_freq == 0:
            self.update_target_network()
        
        self.step_count += 1
        self.losses.append(float(loss))
        
        return float(loss)
    
    def save(self, filepath='./model/'):
        """Sauvegarder le mod√®le"""
        os.makedirs(filepath, exist_ok=True)
        self.main_network.save_weights(os.path.join(filepath, 'cnn_model.weights.h5'))
        
        # Sauvegarder l'√©tat d'entra√Ænement
        np.savez(os.path.join(filepath, 'cnn_state.npz'),
                 episode_count=self.episode_count,
                 step_count=self.step_count,
                 epsilon=self.epsilon,
                 losses=self.losses,
                 scores=self.scores,
                 avg_scores=self.avg_scores)
        
        print(f"üíæ Mod√®le CNN sauvegard√© dans {filepath}")


def train_cnn_agent(episodes=1000):
    """Fonction d'entra√Ænement compl√®te avec solutions"""
    print("üöÄ D√©marrage de l'entra√Ænement CNN avec solutions...")
    
    # Initialisation
    game = SnakeGameAI2(w=336, h=336, image_based=True)
    agent = SimpleDQNAgent(state_shape=(84, 84, 4))
    
    scores = []
    avg_scores = []
    total_score = 0
    record = 0
    
    for episode in range(episodes):
        # R√©initialiser le jeu
        state, _, _, _ = game.reset()
        
        total_reward = 0
        done = False
        
        while not done:
            # Choisir une action
            action = agent.act(state)
            
            # Ex√©cuter l'action
            next_state, reward, done, score = game.play_step([1 if i == action else 0 for i in range(3)])
            
            # Stocker l'exp√©rience
            agent.remember(state, action, reward, next_state, done)
            
            # Passer au state suivant
            state = next_state
            total_reward += reward
            
            # Entra√Æner l'agent
            if len(agent.memory) > agent.min_replay_size:
                loss = agent.train()
        
        # Statistiques
        agent.episode_count += 1
        scores.append(score)
        total_score += score
        avg_score = total_score / (episode + 1)
        avg_scores.append(avg_score)
        
        if score > record:
            record = score
            agent.save()
        
        # Affichage des progr√®s
        if episode % 10 == 0:
            print(f"√âpisode {episode}, Score: {score}, Moyenne: {avg_score:.2f}, "
                  f"Epsilon: {agent.epsilon:.3f}, Record: {record}")
    
    print("üéâ Entra√Ænement termin√© !")
    return agent, scores, avg_scores


def demo_cnn_architecture():
    """D√©monstration de l'architecture CNN"""
    print("üèóÔ∏è D√©monstration de l'architecture CNN")
    
    agent = SimpleDQNAgent()
    
    # Afficher le r√©sum√© du mod√®le
    agent.main_network.summary()
    
    # Test avec une image factice
    dummy_state = np.random.randint(0, 255, (84, 84, 4), dtype=np.uint8)
    processed_state = agent.preprocess_state(dummy_state)
    
    print(f"\nTest avec image factice :")
    print(f"√âtat original shape: {dummy_state.shape}")
    print(f"√âtat trait√© shape: {processed_state.shape}")
    print(f"Valeurs min/max: {processed_state.min():.3f} / {processed_state.max():.3f}")
    
    # Pr√©diction
    action = agent.act(dummy_state)
    print(f"Action pr√©dite: {action}")


if __name__ == '__main__':
    print("üéÆ Solutions CNN + Reinforcement Learning")
    print("\nOptions disponibles :")
    print("1. demo_cnn_architecture() - Voir l'architecture")
    print("2. train_cnn_agent(episodes=100) - Entra√Æner l'agent")
    
    # D√©monstration de l'architecture
    demo_cnn_architecture()
    
    # Entra√Ænement (d√©commentez pour lancer)
    # agent, scores, avg_scores = train_cnn_agent(episodes=100) 