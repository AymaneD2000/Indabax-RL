# üìã Guide de l'Animateur - Workshop CNN + Apprentissage par Renforcement

## üéØ Vue d'ensemble du Workshop Avanc√©

**Dur√©e estim√©e :** 4-6 heures  
**Niveau :** Interm√©diaire √† Avanc√© en IA/ML  
**Pr√©requis :** Bases CNN, TensorFlow, notions d'apprentissage par renforcement

## ‚ö° Diff√©rences avec le Workshop Basique

### **Complexit√© Accrue**
| Aspect | Workshop Basique | Workshop CNN |
|--------|------------------|--------------|
| **Public cible** | D√©butants ML | Interm√©diaires CNN |
| **Concepts** | RL fondamental | RL + Vision |
| **Temps d'ex√©cution** | 5-10 min | 30-60 min |
| **D√©bogage** | Simple | Complexe (dimensions, m√©moire) |

## üìä Structure Recommand√©e

### **Phase 1 : Introduction Avanc√©e (45-60 min)**

#### **1. Rappel Rapide RL (10 min)**
- Q-Learning, epsilon-greedy, exp√©rience replay
- *"Vous connaissez d√©j√† ces concepts"*

#### **2. Introduction CNN pour RL (20 min)**
- **Pourquoi les images ?** : Vision humaine vs algorithmes
- **D√©fis sp√©cifiques** : Dimensions, m√©moire, temps de calcul
- **Avantages** : G√©n√©ralisation, r√©alisme

#### **3. Architecture CNN (15 min)**
- **Convolution** : D√©tection de motifs locaux
- **Stride et padding** : R√©duction de taille
- **Feature maps** : Repr√©sentations abstraites
- **Pipeline compl√®te** : Image ‚Üí Features ‚Üí D√©cision

### **Phase 2 : D√©monstration Technique (20 min)**

#### **Analyse du Projet**
- Comparaison avec le projet basique
- Traitement d'images optimis√©
- Frame stacking et preprocessing

### **Phase 3 : Travaux Pratiques (3h30-4h)**

## üîÑ Ordre d'Ex√©cution Recommand√©

### **EXERCICE 1 : Architecture CNN (60-75 min) - AVANC√â**

#### **Points d'Attention Critiques :**
- **Dimensions** : Input (84,84,4) ‚Üí Output (3)
- **Normalisation** : Lambda layer obligatoire
- **Stride vs Pooling** : Stride pour r√©duction de taille
- **Activation finale** : Linear pour Q-values

#### **Erreurs Fr√©quentes :**
```python
# ‚ùå ERREUR : Oublier la normalisation
tf.keras.layers.Conv2D(16, 8, strides=4)

# ‚úÖ CORRECT : Avec normalisation
tf.keras.layers.Lambda(lambda x: tf.cast(x, tf.float32) / 255.0),
tf.keras.layers.Conv2D(16, 8, strides=4, activation='relu')
```

#### **Aide Progressive :**
1. "Commencez par Input et Lambda pour normalisation"
2. "Conv2D prend (filtres, kernel_size, strides, activation)"
3. "N'oubliez pas Flatten() avant Dense"

---

### **EXERCICE 2 : Preprocessing (30-40 min) - INTERM√âDIAIRE**

#### **Points Cl√©s :**
- **Validation des formes** : Critique pour √©viter les crashes
- **Types de donn√©es** : float32 obligatoire pour TensorFlow
- **Gestion des plages** : 0-255 vs 0-1

#### **Code Type √† Expliquer :**
```python
if state.shape != self.state_shape:
    raise ValueError(f"Attendu {self.state_shape}, re√ßu {state.shape}")

state = state.astype(np.float32)
if np.max(state) > 1.0:
    state = state / 255.0
```

---

### **EXERCICE 3 : Epsilon-Greedy Images (45-60 min) - INTERM√âDIAIRE**

#### **Nouveaut√©s vs Version Basique :**
- **Preprocessing int√©gr√©** dans la pipeline
- **Gestion des dimensions** : expand_dims pour batch
- **TensorFlow tensor** au lieu de numpy

#### **S√©quence Critique :**
```python
processed_state = self.preprocess_state(state)  # ‚úÖ
state_tensor = tf.constant(np.expand_dims(processed_state, axis=0))  # ‚úÖ
q_values = self.main_network(state_tensor, training=False)  # ‚úÖ
return int(tf.argmax(q_values[0]).numpy())  # ‚úÖ
```

---

### **EXERCICE 4 : Entra√Ænement CNN (75-90 min) - TR√àS AVANC√â**

#### **D√©fis Majeurs :**
- **Gestion m√©moire** : Arrays d'images volumineuses
- **Dimensions complexes** : 4D tensors
- **Performance** : Beaucoup plus lent que dense

#### **Points Critiques √† Surveiller :**
```python
# Gestion des types de donn√©es
states = np.array([e[0] for e in batch], dtype=np.float32)  # ‚ö†Ô∏è Obligatoire
actions = np.array([e[1] for e in batch], dtype=np.int32)   # ‚ö†Ô∏è Obligatoire

# S√©lection des Q-values pour actions prises
batch_indices = tf.range(tf.shape(actions)[0], dtype=tf.int32)
action_indices = tf.stack([batch_indices, actions], axis=1)
current_q_values = tf.gather_nd(current_q_values, action_indices)
```

## üÜò Gestion des Probl√®mes Sp√©cifiques

### **Probl√®mes de Performance**

#### **"L'entra√Ænement est trop lent"**
```python
# Solutions imm√©diates :
batch_size = 16  # Au lieu de 32
memory_size = 500  # Au lieu de 1000
SPEED = 400  # Augmenter la vitesse du jeu
```

#### **"M√©moire insuffisante"**
```python
# R√©ductions n√©cessaires :
state_shape = (42, 42, 4)  # Au lieu de (84, 84, 4)
batch_size = 8  # Tr√®s petit batch
```

### **Probl√®mes de Debugging**

#### **"Formes incompatibles"**
```python
# Debug syst√©matique :
print(f"State shape: {state.shape}")
print(f"Expected: {agent.state_shape}")
print(f"State dtype: {state.dtype}")
print(f"State min/max: {state.min()}/{state.max()}")
```

#### **"Pas d'am√©lioration"**
- **Epsilon trop √©lev√©** : V√©rifier `get_epsilon()`
- **Learning rate** : Essayer 0.001 ou 0.0001
- **Target update** : V√©rifier la fr√©quence

### **Erreurs TensorFlow Communes**

#### **Type Mismatch**
```python
# ‚ùå ERREUR
states = np.array([e[0] for e in batch])  # dtype par d√©faut

# ‚úÖ SOLUTION  
states = np.array([e[0] for e in batch], dtype=np.float32)
```

#### **Dimension Errors**
```python
# Debug dimensions :
print(f"States tensor shape: {states_tensor.shape}")  # Doit √™tre (batch, 84, 84, 4)
print(f"Q-values shape: {current_q_values.shape}")    # Doit √™tre (batch, 3)
```

## üéØ Adaptations selon Niveau

### **Groupe Interm√©diaire**
- **Plus de temps** sur concepts CNN (45 min th√©orie)
- **Exercice 1 guid√©** √©tape par √©tape
- **Focus sur debugging** plut√¥t que optimisations

### **Groupe Avanc√©**
- **Th√©orie acc√©l√©r√©e** (30 min)
- **D√©fis bonus** : Double DQN, Dueling DQN
- **Exp√©rimentations** : Hyperparam√®tres, architectures

## üìä Monitoring de l'Entra√Ænement

### **M√©triques Critiques √† Surveiller**

#### **Pendant l'Entra√Ænement :**
```python
# Affichage recommand√© :
print(f"√âpisode {episode}")
print(f"Score: {score}, Moyenne: {avg_score:.2f}")
print(f"Epsilon: {agent.epsilon:.3f}")
print(f"Memory size: {len(agent.memory)}")
print(f"Loss: {loss:.4f}" if loss else "Training not started")
```

#### **Signaux de Sant√© :**
- **Epsilon** : Diminue r√©guli√®rement de 1.0 vers 0.01
- **Perte** : D√©croissance g√©n√©rale (peut fluctuer)
- **Score moyen** : Augmentation lente mais constante
- **Memory** : Atteint min_replay_size rapidement

## ‚è±Ô∏è Gestion du Temps

### **Timeline R√©aliste :**
- **Exercice 1** : 60-75 min (le plus complexe)
- **Exercice 2** : 30-40 min (plus simple)
- **Exercice 3** : 45-60 min (int√©gration)
- **Exercice 4** : 75-90 min (le plus difficile)
- **Test/Demo** : 30-45 min

### **Si Retard :**
1. **Simplifier Ex1** : Donner l'architecture, expliquer seulement
2. **Acc√©l√©rer Ex2** : Code ensemble au tableau
3. **Focus Ex3-4** : C≈ìur du workshop

## üéÆ Phase de Test et D√©monstration

### **R√©sultats Attendus :**
- **0-50 √©pisodes** : Scores tr√®s bas (0-2), mouvement al√©atoire
- **50-100 √©pisodes** : Premiers apprentissages (2-5)
- **100-200 √©pisodes** : √âvite les murs consistentement (5-8)
- **200+ √©pisodes** : Strat√©gies √©mergentes (8-15)

### **Comportements √† Pointer :**
- **Exploration massive** au d√©but
- **Graduelle exploitation** avec am√©lioration
- **√âmergence de strat√©gies** : spirales, contournement

## üí° Questions Avanc√©es des Participants

### **"Pourquoi 84x84x4 ?"**
- **84x84** : Standard Atari DQN (√©quilibre r√©solution/performance)
- **4 frames** : Perception du mouvement et de la direction

### **"Pourquoi grayscale ?"**
- **R√©duction dimensionnelle** : 3 canaux ‚Üí 1 canal
- **Information suffisante** : Formes plus importantes que couleurs
- **Performance** : 3x moins de param√®tres

### **"Diff√©rence avec classification d'images ?"**
- **Objectif** : Actions optimales vs cat√©gorisation
- **Feedback** : R√©compenses vs labels
- **Temporalit√©** : S√©quences vs images isol√©es

## üèÜ Extensions et Ouvertures

### **Am√©liorations Techniques :**
- **Double DQN** : R√©duction du biais Q-value
- **Dueling DQN** : S√©paration valeur/avantage  
- **Prioritized Experience Replay** : √âchantillonnage intelligent

### **Autres Applications :**
- **Jeux Atari** : Adaptation directe
- **Robotique** : Vision pour navigation
- **V√©hicules autonomes** : Perception visuelle

---

**Bonne Animation de Workshop Avanc√© ! üöÄüß†**

*N'oubliez pas : CNN + RL = r√©volution de l'IA moderne !* 