# ğŸ§  Workshop AvancÃ© : CNN + Apprentissage par Renforcement

Bienvenue dans ce workshop avancÃ© ! Vous allez apprendre Ã  crÃ©er un agent qui "voit" le jeu Ã  travers des images, exactement comme le ferait un humain, en utilisant les **RÃ©seaux de Neurones Convolutionnels (CNN)**.

## ğŸ¯ Objectifs PÃ©dagogiques

Ã€ la fin de ce workshop, vous maÃ®triserez :
- **Architecture CNN** pour la vision par ordinateur
- **Traitement d'images** pour l'IA
- **Deep Q-Learning** avec des entrÃ©es visuelles
- **Experience Replay** pour l'apprentissage par renforcement
- **Target Networks** pour stabiliser l'entraÃ®nement

## ğŸ”¥ NouveautÃ©s par rapport au Workshop PrÃ©cÃ©dent

### **Ã‰volution Majeure : De Vecteurs Ã  Images**
| Aspect | Workshop Basique | Workshop CNN |
|--------|------------------|--------------|
| **EntrÃ©e** | 11 nombres (dangers, directions) | Images 84x84x4 |
| **RÃ©seau** | Dense (256 neurones) | CNN (Conv2D + Dense) |
| **Preprocessing** | Aucun | Redimensionnement, Grayscale |
| **ComplexitÃ©** | â­â­ | â­â­â­â­â­ |
| **Temps d'entraÃ®nement** | ~5 minutes | ~30-60 minutes |

## ğŸ—ï¸ Architecture du Projet

```
â”œâ”€â”€ game.py              # âœ… Jeu Snake optimisÃ© pour CNN (complet)
â”œâ”€â”€ agent_workshop.py    # ğŸ”§ Agent CNN Ã  complÃ©ter (4 EXERCICES)
â”œâ”€â”€ solutions/           # ğŸ’¡ Solutions complÃ¨tes
â””â”€â”€ README_WORKSHOP_CNN.md
```

## ğŸ“š Les 4 Exercices AvancÃ©s

### ğŸ¥‡ **EXERCICE 1 : Architecture CNN** (â˜…â˜…â˜… AvancÃ©)
**Fichier :** `agent_workshop.py` â†’ fonction `_build_network()`

**Mission :** CrÃ©er un CNN qui transforme des images en dÃ©cisions.

**Concepts clÃ©s :**
- **Convolution** : DÃ©tection de motifs locaux
- **Pooling implicite** : RÃ©duction de taille avec stride
- **Feature maps** : ReprÃ©sentations abstraites des images

**Architecture Ã  implÃ©menter :**
```
Input (84, 84, 4) â†’ Normalisation â†’ Conv2D â†’ Conv2D â†’ Conv2D â†’ Dense â†’ Output (3)
```

---

### ğŸ¥ˆ **EXERCICE 2 : PrÃ©processing d'Images** (â˜…â˜…â˜† IntermÃ©diaire)
**Fichier :** `agent_workshop.py` â†’ fonction `preprocess_state()`

**Mission :** PrÃ©parer les images pour l'entrÃ©e du CNN.

**Concepts clÃ©s :**
- **Normalisation** : 0-255 â†’ 0-1
- **Validation des formes** : (84, 84, 4)
- **Types de donnÃ©es** : float32 pour TensorFlow

---

### ğŸ¥‰ **EXERCICE 3 : Epsilon-Greedy pour Images** (â˜…â˜…â˜† IntermÃ©diaire)
**Fichier :** `agent_workshop.py` â†’ fonction `act()`

**Mission :** Adapter la stratÃ©gie d'exploration pour les images.

**NouveautÃ©s :**
- **Dimensions** : Gestion des images 4D
- **Preprocessing** : IntÃ©gration dans la pipeline
- **PrÃ©diction** : CNN au lieu de rÃ©seau dense

---

### ğŸ† **EXERCICE 4 : EntraÃ®nement CNN** (â˜…â˜…â˜… AvancÃ©)
**Fichier :** `agent_workshop.py` â†’ fonction `train()`

**Mission :** Optimiser un CNN avec l'expÃ©rience replay.

**DÃ©fis spÃ©cifiques :**
- **MÃ©moire** : Images vs vecteurs
- **Batch processing** : Gestion des dimensions
- **Target Networks** : Stabilisation de l'entraÃ®nement

## ğŸš€ PrÃ©-requis Techniques

### **BibliothÃ¨ques NÃ©cessaires**
```bash
pip install tensorflow opencv-python numpy pygame matplotlib
```

### **Connaissances RecommandÃ©es**
- **Bases CNN** : Convolution, pooling, feature maps
- **TensorFlow/Keras** : Couches, optimiseurs, gradient tape
- **Traitement d'images** : Redimensionnement, normalisation
- **Apprentissage par renforcement** : Q-Learning, epsilon-greedy

## ğŸ® DiffÃ©rences avec le Jeu Basique

### **Optimisations AvancÃ©es**

#### **Couleurs OptimisÃ©es pour CNN**
- **ArriÃ¨re-plan** : Noir (0) â†’ Valeur grayscale minimale
- **Nourriture** : Rouge (255,0,0) â†’ ~76 en grayscale
- **Corps serpent** : Gris (180,180,180) â†’ ~180 en grayscale  
- **TÃªte serpent** : Blanc (255,255,255) â†’ 255 en grayscale

#### **Frame Stacking**
- **4 images consÃ©cutives** empilÃ©es
- **Perception du mouvement** comme un humain
- **Historique temporel** pour meilleures dÃ©cisions

#### **Preprocessing AvancÃ©**
- **Capture d'Ã©cran** sans texte
- **Redimensionnement** 336x336 â†’ 84x84
- **Conversion grayscale** optimisÃ©e
- **Normalisation** 0-255 â†’ 0-1

## ğŸ’¡ Conseils pour RÃ©ussir

### **ğŸ” DÃ©bogage CNN**
```python
# VÃ©rifier les formes d'entrÃ©e
print(f"State shape: {state.shape}")  # Doit Ãªtre (84, 84, 4)

# VÃ©rifier les prÃ©dictions
q_values = model.predict(state_batch)
print(f"Q-values: {q_values}")  # 3 valeurs par action
```

### **ğŸ“Š Surveillance de l'entraÃ®nement**
- **Perte** : Doit diminuer progressivement
- **Epsilon** : DÃ©croÃ®t de 1.0 Ã  0.01
- **Score moyen** : AmÃ©lioration lente mais constante
- **Temps** : Patience ! CNN = plus lent que dense

### **âš¡ Optimisations Performance**
- **Batch size** : 32 (Ã©quilibre mÃ©moire/vitesse)
- **Target update** : Tous les 100 steps
- **Memory size** : 1000 expÃ©riences max
- **Frame rate** : 200 FPS pour entraÃ®nement rapide

## ğŸ§ª ExpÃ©rimentations Possibles

### **Variations d'Architecture**
- **Plus de couches** : Conv2D supplÃ©mentaires
- **Filtres diffÃ©rents** : 64, 128 au lieu de 32
- **Kernel sizes** : 5x5, 7x7 au lieu de 3x3
- **Dropout** : RÃ©gularisation pour Ã©viter l'overfitting

### **HyperparamÃ¨tres**
- **Learning rate** : 0.001, 0.0001
- **Gamma** : 0.95, 0.99
- **Epsilon decay** : Plus rapide ou plus lent
- **Batch size** : 16, 64, 128

## ğŸ”¬ Analyse des RÃ©sultats

### **MÃ©triques Ã  Observer**
- **Score maximum** : Record absolu atteint
- **Score moyen** : Tendance gÃ©nÃ©rale
- **Perte** : Convergence de l'apprentissage
- **Epsilon** : Ã‰volution explorationâ†’exploitation

### **Comportements Ã‰mergents**
- **Premiers Ã©pisodes** : Mouvement alÃ©atoire
- **50-100 Ã©pisodes** : Ã‰vite les murs
- **200-300 Ã©pisodes** : Cherche la nourriture
- **500+ Ã©pisodes** : StratÃ©gies optimales

## âš ï¸ RÃ©solution de ProblÃ¨mes

### **Erreurs Communes**
```python
# Forme incorrecte
ValueError: Input shape mismatch â†’ VÃ©rifiez (84, 84, 4)

# MÃ©moire insuffisante
OOM error â†’ RÃ©duisez batch_size ou memory_size

# Pas d'amÃ©lioration
â†’ VÃ©rifiez epsilon decay et learning rate
```

### **Signaux d'Alerte**
- **Perte qui explose** : Learning rate trop Ã©levÃ©
- **Score qui stagne** : Epsilon trop bas trop tÃ´t
- **EntraÃ®nement trop lent** : CPU au lieu de GPU

## ğŸ RÃ©sultats Attendus

### **AprÃ¨s 500 Ã‰pisodes**
- **Score moyen** : 5-10 points
- **Score maximum** : 15-25 points
- **Comportement** : Navigation fluide, recherche active de nourriture

### **Comparaison avec Humain**
- **Humain novice** : ~10-15 points
- **Agent entraÃ®nÃ©** : ~15-25 points
- **Avantage agent** : RÃ©flexes parfaits, pas de fatigue

## ğŸ”® Extensions Possibles

### **Algorithmes AvancÃ©s**
- **Double DQN** : RÃ©duction du biais d'estimation
- **Dueling DQN** : SÃ©paration valeur/avantage
- **Rainbow DQN** : Combinaison de toutes les amÃ©liorations

### **Autres Jeux**
- **Atari** : Breakout, Pong, Space Invaders
- **Jeux personnalisÃ©s** : Adaptation de l'architecture
- **Environnements 3D** : Minecraft, Unity

---

**ğŸ‰ Bon Workshop et Bonne DÃ©couverte de la Vision Artificielle !**

*Rappelez-vous : Les CNNs rÃ©volutionnent l'IA en permettant aux machines de "voir" le monde comme nous !* ğŸ¤–ğŸ‘ï¸ 