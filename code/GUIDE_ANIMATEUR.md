# üìã Guide de l'Animateur - Workshop Apprentissage par Renforcement

## üéØ Vue d'ensemble du Workshop

**Dur√©e estim√©e :** 3-4 heures  
**Niveau :** D√©butant √† Interm√©diaire en IA/ML  
**Pr√©requis :** Bases de Python, notions de machine learning optionnelles

## üìä Structure Recommand√©e

### **Phase 1 : Introduction Th√©orique (30-45 min)**

#### Concepts √† Pr√©senter :
1. **Qu'est-ce que l'Apprentissage par Renforcement ?**
   - Agent, Environnement, Actions, √âtats, R√©compenses
   - Diff√©rence avec supervised/unsupervised learning
   - Exemples concrets : jeux vid√©o, robots, voitures autonomes

2. **Deep Q-Learning (DQN)**
   - Probl√®me : espaces d'√©tats √©normes 
   - Solution : approximation avec r√©seaux de neurones
   - Q-values : "qualit√©" d'une action dans un √©tat donn√©

3. **Strat√©gie Epsilon-Greedy**
   - Dilemme exploration vs exploitation
   - Epsilon d√©croissant au fil du temps

4. **√âquation de Bellman**
   - `Q(s,a) = reward + gamma * max(Q(s',a'))`
   - R√©compense imm√©diate + potentiel futur

### **Phase 2 : Pr√©sentation du Projet (15 min)**

#### D√©monstration :
- Montrer le code original fonctionnel
- Lancer l'entra√Ænement pour quelques √©pisodes
- Expliquer l'architecture des fichiers
- Pr√©senter les fichiers "workshop" √† compl√©ter

### **Phase 3 : Travaux Pratiques (2h30-3h)**

## üîÑ Ordre d'Ex√©cution des Exercices

### **EXERCICE 1 : √âtat (45 min) - D√âBUTANT** 
**Pourquoi commencer par l√† :** C'est la base conceptuelle

#### **Points d'Attention :**
- Bien faire comprendre que l'agent ne "voit" que des nombres
- Les 11 √©l√©ments bool√©ens : 3 dangers + 4 directions + 4 positions nourriture
- Aide commune : confusion entre directions absolues et relatives

#### **Indices √† Donner Progressivement :**
1. "Commencez par r√©cup√©rer `head = game.snake[0]`"
2. "Les points potentiels sont √† 20 pixels de distance (taille d'un bloc)"
3. "Pour les dangers, pensez aux directions relatives : droit devant, √† droite, √† gauche"

#### **Erreurs Fr√©quentes :**
- Oublier `np.array(state, dtype=int)` en retour
- Mal g√©rer les directions relatives vs absolues
- √âtat avec mauvais nombre d'√©l√©ments

---

### **EXERCICE 4 : R√©seau de Neurones (30 min) - INTERM√âDIAIRE**
**Pourquoi apr√®s l'√©tat :** L'architecture est plus simple √† comprendre

#### **Points d'Attention :**
- ReLU pour la couche cach√©e, pas d'activation pour la sortie
- Importance du nombre de neurones : entr√©e=11, sortie=3

#### **Indices √† Donner :**
1. "`tf.keras.layers.Dense(hidden_size, activation='relu')`"
2. "Dans `call()`, c'est juste `x = self.dense1(input)` puis `return self.dense2(x)`"

#### **Erreurs Fr√©quentes :**
- Activation sur la couche de sortie
- Oublier le `return` dans `call()`

---

### **EXERCICE 2 : Actions Epsilon-Greedy (45 min) - INTERM√âDIAIRE**
**Pourquoi maintenant :** N√©cessite la compr√©hension de l'√©tat et du mod√®le

#### **Points d'Attention :**
- Epsilon d√©cro√Æt : beaucoup d'exploration au d√©but, moins apr√®s
- `final_move` : vecteur one-hot [1,0,0], [0,1,0], ou [0,0,1]
- `np.argmax()` pour trouver la meilleure action pr√©dite

#### **Algorithme √† Expliquer au Tableau :**
```
SI random < epsilon :
    action = al√©atoire
SINON :
    pr√©dictions = mod√®le(√©tat)
    action = argmax(pr√©dictions)
```

#### **Erreurs Fr√©quentes :**
- Ne pas r√©initialiser `final_move = [0,0,0]`
- Confondre `randint(0,2)` et `randint(0,3)`
- Mauvais formatage de l'√©tat pour le mod√®le

---

### **EXERCICE 3 : √âquation de Bellman (45 min) - AVANC√â**
**Pourquoi en dernier :** Le plus conceptuellement difficile

#### **Points d'Attention :**
- C'est le c≈ìur th√©orique de l'apprentissage par renforcement
- Distinction cas terminal vs non-terminal
- `np.argmax(action[idx])` trouve quelle action a √©t√© prise

#### **Explication D√©taill√©e N√©cessaire :**
1. **Si √©pisode termin√© :** `Q_new = reward` (pas de futur)
2. **Si √©pisode continue :** `Q_new = reward + gamma * max(Q_next)`
3. **Mise √† jour selective :** seule la Q-value de l'action prise est modifi√©e

#### **Erreurs Fr√©quentes :**
- Oublier le cas `if not done[idx]`
- `np.max(new_pred[idx])` vs `np.max(new_pred)`
- `np.argmax(action[idx])` pour trouver l'index de l'action

## üÜò Gestion des Difficult√©s

### **Si les Participants sont Bloqu√©s :**
1. **Premier niveau :** Hints progressifs dans les commentaires
2. **Deuxi√®me niveau :** Montrer la solution d'une partie sp√©cifique
3. **Dernier recours :** Fichiers solutions complets dans `/solutions/`

### **Si le Code ne Marche Pas :**
- V√©rifier que tous les imports sont corrects dans les fichiers workshop
- S'assurer que `pygame` et `tensorflow` sont install√©s
- Les `pass` doivent √™tre remplac√©s par du code

### **Adaptations selon le Niveau :**

#### **Groupe D√©butant :**
- Plus de temps sur la th√©orie
- Faire l'Exercice 1 ensemble au tableau
- Simplifier l'Exercice 3 (donner plus d'indices)

#### **Groupe Avanc√© :**
- Moins de temps sur la th√©orie
- D√©fis bonus : modifier les r√©compenses, l'architecture r√©seau
- Discussion sur autres algorithmes RL (A3C, PPO, etc.)

## üéÆ Phase de Test et Observation

### **Apr√®s Impl√©mentation Compl√®te :**
1. **Lancer l'entra√Ænement ensemble**
2. **Observer les premi√®res parties :** mouvement al√©atoire
3. **Apr√®s 50-100 parties :** premiers apprentissages
4. **Apr√®s 200+ parties :** strat√©gies optimales

### **Points √† Faire Remarquer :**
- Epsilon qui diminue ‚Üí moins d'exploration
- Score moyen qui augmente
- Strat√©gies √©mergentes (longer les murs, spirales, etc.)

## üí° Questions Fr√©quentes des Participants

### **"Pourquoi le serpent ne s'am√©liore pas ?"**
- V√©rifier que l'√©quation de Bellman est correcte
- L'apprentissage prend du temps (100-500 parties)
- Epsilon trop √©lev√© = trop d'exploration

### **"√Ä quoi sert gamma ?"**
- Contr√¥le l'importance du futur vs pr√©sent
- Gamma = 0.9 ‚Üí 90% d'importance pour les r√©compenses futures
- Gamma proche de 1 ‚Üí vision long terme

### **"Pourquoi replay buffer ?"**
- √âviter la corr√©lation entre exp√©riences cons√©cutives
- Utiliser efficacement toutes les exp√©riences pass√©es
- Stabiliser l'apprentissage

## üèÜ Conclusion et Ouverture (15 min)

### **R√©capitulatif des Concepts :**
- Repr√©sentation d'√©tat
- Epsilon-greedy
- √âquation de Bellman
- Deep Q-Learning

### **Extensions Possibles :**
- Autres jeux (Pong, Atari, etc.)
- Autres algorithmes (Policy Gradient, Actor-Critic)
- Applications r√©elles (robotique, finance, etc.)

### **Ressources pour Aller Plus Loin :**
- Cours en ligne (Coursera, edX)
- Biblioth√®ques (OpenAI Gym, Stable-Baselines3)
- Papiers de recherche (DeepMind, OpenAI)

---

**Bon Workshop ! üöÄ** 