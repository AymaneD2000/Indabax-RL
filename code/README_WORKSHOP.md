# ğŸ Workshop : Apprentissage par Renforcement avec Snake Game

Bienvenue dans ce workshop d'introduction Ã  l'apprentissage par renforcement ! Vous allez apprendre les concepts clÃ©s en implÃ©mentant un agent qui apprend Ã  jouer au jeu Snake.

## ğŸ¯ Objectifs PÃ©dagogiques

Ã€ la fin de ce workshop, vous comprendrez :
- Les concepts fondamentaux de l'apprentissage par renforcement
- La reprÃ©sentation d'Ã©tat dans un environnement de jeu
- La stratÃ©gie epsilon-greedy (exploration vs exploitation)
- L'Ã©quation de Bellman et le Q-Learning
- L'architecture des rÃ©seaux de neurones pour l'IA

## ğŸ—ï¸ Structure du Projet

```
â”œâ”€â”€ game.py              # âœ… Le jeu Snake (complet)
â”œâ”€â”€ helper.py            # âœ… Visualisation des scores (complet)
â”œâ”€â”€ agent_workshop.py    # ğŸ”§ Agent Ã  complÃ©ter (EXERCICES 1-2)
â”œâ”€â”€ trainer_workshop.py  # ğŸ”§ EntraÃ®neur Ã  complÃ©ter (EXERCICE 3)
â”œâ”€â”€ model_workshop.py    # ğŸ”§ RÃ©seau de neurones Ã  complÃ©ter (EXERCICE 4)
â””â”€â”€ solutions/           # ğŸ’¡ Solutions complÃ¨tes (Ã  consulter si bloquÃ©)
```

## ğŸ“š Exercices Ã  RÃ©aliser

### ğŸ¥‰ **EXERCICE 1 : ReprÃ©sentation de l'Ã‰tat** (DÃ©butant)
**Fichier :** `agent_workshop.py` â†’ fonction `getState()`

**Mission :** CrÃ©er une reprÃ©sentation numÃ©rique de l'Ã©tat du jeu.

**Concepts :** L'agent doit "voir" son environnement sous forme de donnÃ©es numÃ©riques.

**Ã€ implÃ©menter :**
- DÃ©tecter les dangers (collisions potentielles)
- Identifier la direction actuelle du serpent
- Localiser la nourriture par rapport Ã  la tÃªte

---

### ğŸ¥ˆ **EXERCICE 2 : StratÃ©gie Epsilon-Greedy** (IntermÃ©diaire)
**Fichier :** `agent_workshop.py` â†’ fonction `getAction()`

**Mission :** Ã‰quilibrer exploration (actions alÃ©atoires) et exploitation (utiliser le modÃ¨le).

**Concepts :** Comment un agent apprend-il de nouvelles stratÃ©gies tout en optimisant celles qu'il connaÃ®t ?

**Ã€ implÃ©menter :**
- Calcul d'epsilon dÃ©croissant
- DÃ©cision exploration vs exploitation
- Utilisation du modÃ¨le pour prÃ©dire les meilleures actions

---

### ğŸ¥‡ **EXERCICE 3 : Ã‰quation de Bellman** (AvancÃ©)
**Fichier :** `trainer_workshop.py` â†’ fonction `trainer()`

**Mission :** ImplÃ©menter le cÅ“ur thÃ©orique de l'apprentissage par renforcement.

**Concepts :** Comment Ã©valuer la qualitÃ© d'une action en combinant rÃ©compense immÃ©diate et potentiel futur ?

**Ã€ implÃ©menter :**
- L'Ã©quation : `Q(s,a) = reward + gamma * max(Q(s',a'))`
- Gestion des Ã©pisodes terminÃ©s
- Mise Ã  jour des Q-values

---

### ğŸ† **EXERCICE 4 : Architecture du RÃ©seau** (IntermÃ©diaire)
**Fichier :** `model_workshop.py` â†’ classe `QNetwork`

**Mission :** CrÃ©er un rÃ©seau de neurones pour approximer la fonction Q.

**Concepts :** Comment transformer un Ã©tat de jeu en Ã©valuation d'actions ?

**Ã€ implÃ©menter :**
- Couches denses avec TensorFlow
- Fonction d'activation ReLU
- Propagation avant des donnÃ©es

## ğŸš€ Comment DÃ©marrer

### 1. **Installation des DÃ©pendances**
```bash
pip install tensorflow numpy pygame matplotlib
```

### 2. **Ordre RecommandÃ© des Exercices**
1. Commencez par l'**Exercice 1** (Ã‰tat) - fondamental
2. Passez Ã  l'**Exercice 4** (RÃ©seau) - architecture de base  
3. Continuez avec l'**Exercice 2** (Action) - logique de dÃ©cision
4. Terminez par l'**Exercice 3** (Bellman) - le plus complexe

### 3. **Test de Votre Code**
```bash
python agent_workshop.py
```

## ğŸ’¡ Conseils pour RÃ©ussir

### ğŸ” **DÃ©bogage**
- Utilisez `print()` pour vÃ©rifier vos valeurs
- L'Ã©tat doit avoir 11 Ã©lÃ©ments boolÃ©ens (True/False)
- Les actions sont des listes [1,0,0], [0,1,0], ou [0,0,1]

### ğŸ“– **Ressources d'Aide**
- Lisez attentivement les commentaires dans chaque exercice
- Les indices sont fournis Ã©tape par Ã©tape
- Consultez les fichiers originaux (`agent.py`, etc.) si vraiment bloquÃ©

### ğŸ® **Observation de l'EntraÃ®nement**
- Au dÃ©but : le serpent bouge alÃ©atoirement (exploration)
- Progressivement : il apprend Ã  Ã©viter les murs
- Finalement : il dÃ©veloppe des stratÃ©gies pour attraper la nourriture

## ğŸ§  Concepts ClÃ©s Ã  Retenir

### **Apprentissage par Renforcement**
- Agent, environnement, actions, Ã©tats, rÃ©compenses
- Objectif : maximiser les rÃ©compenses cumulÃ©es

### **Deep Q-Learning**
- Utilise un rÃ©seau de neurones pour approximer Q(Ã©tat, action)
- MÃ©morise les expÃ©riences passÃ©es (replay buffer)
- StratÃ©gie epsilon-greedy pour l'exploration

### **Ã‰quation de Bellman**
- Relie la valeur d'une action Ã  la rÃ©compense immÃ©diate + potentiel futur
- Facteur gamma : importance des rÃ©compenses futures
- Base thÃ©orique de tous les algorithmes de Q-Learning

## ğŸ RÃ©sultats Attendus

AprÃ¨s implÃ©mentation correcte :
- Le serpent apprend Ã  Ã©viter les murs et son propre corps
- Le score moyen augmente progressivement
- L'agent dÃ©veloppe des stratÃ©gies optimales

## ğŸ“ Aide et Support

- **Erreurs de syntaxe :** VÃ©rifiez l'indentation Python
- **Erreurs de logique :** Relisez les indices dans les commentaires
- **Performances :** L'apprentissage peut prendre 100-500 parties
- **Questions :** N'hÃ©sitez pas Ã  demander de l'aide !

---

**ğŸ‰ Bon Workshop et Bon Apprentissage !**

*Rappelez-vous : L'apprentissage par renforcement imite notre faÃ§on d'apprendre - par essais, erreurs, et amÃ©lioration continue !* 